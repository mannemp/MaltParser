compute-1-9.hpc.engr.oregonstate.edu
-----------------------------------------------------------------------------
                   MaltOptimizer 1.0
-----------------------------------------------------------------------------
         Miguel Ballesteros* and Joakim Nivre**

          *Complutense University of Madrid (Spain)  
                **Uppsala University (Sweden)   
-----------------------------------------------------------------------------
PHASE 1: DATA ANALYSIS
In order to optimize MaltParser for your training set, MaltOptimizer will 
first analyze the data and set some basic parameters.
-----------------------------------------------------------------------------
DATA VALIDATION
Validating the CoNLL data format ...  (may take a few minutes)
Your training set is in valid CoNLL format.
-----------------------------------------------------------------------------
DATA CHARACTERISTICS
Your training set consists of 41178 sentences and 983221 tokens.
Testing Java Heap ... 
MaltOptimizer has inferred that MaltParser needs at least 20Gb of free memory.
CPOSTAG and POSTAG are distinct in your training set.
The LEMMA column is used in your training set.
The FEATS column is not used in your training set.
Your training set contains a small amount of non-projective trees (7.703 %).
Your training set does not contain unattached internal punctuation.
Your training set has a unique DEPREL label for tokens where HEAD=0:ROOT.
-----------------------------------------------------------------------------
BASIC OPTIMIZATION SETUP 
Generating training and test files for optimization...
Five cross-validation folds generated.
Generated training set (817874 tokens) and devtest set (206524 tokens).
Given that your data set is relatively large, we recommend using a single 
development set during subsequent optimization phases. If you prefer to use 5-fold cross-validation, you can specify this instead (-v cv).
Testing the default settings ... (may take a few seconds)
LAS with default settings: 85.72%
-----------------------------------------------------------------------------
MaltOptimizer has completed the analysis of your training set and saved the
results for future use in /phase1_logFile.txt. Updated MaltParser options can be found
in /phase1_optFile.txt. If you want to change any of these options, you should
edit /phase1_optFile.txt before you start the next optimization phase.

To proceed with Phase 2 (Parsing Algorithm) run the following command:
java -jar MaltOptimizer.jar -p 2 -m <malt_path> -c <trainingCorpus>
-----------------------------------------------------------------------------
                   MaltOptimizer 1.0
-----------------------------------------------------------------------------
         Miguel Ballesteros* and Joakim Nivre**

          *Complutense University of Madrid (Spain)  
                **Uppsala University (Sweden)   
-----------------------------------------------------------------------------
PHASE 2: PARSING ALGORITHM SELECTION

MaltOptimizer found in Phase 1 that your training set contains
a small amount of non-projective trees and will therefore 
try both projective and non-projective algorithms.

Testing projective algorithms ...

                       NivreEager --vs-- StackProjective
                           /                  \
                          /                    \
                         /                      \
                        /                        \
                       /                          \
                      /                            \
                     /                              \
            CovingtonProjective                 NivreStandard


Testing the NivreEager algorithm ...
Testing the StackProjective algorithm ...
Testing the Covington--Projective algorithm ...
Best projective algorithm: NivreEager

Testing the non-projective algorithms ...

               CovingtonNonProjective --vs-- StackLazy
                          /                     \
                         /                       \
                        /                         \
                       /                           \
                      /                             \
                     /                               \
                    /                                 \
               NivreEager+PP             StackEager --vs-- StackProjective+PP
                    |                                  |
                    |                                  |
         CovingtonProjective+PP                 NivreStandard+PP


Testing the Covington-Non-Projective algorithm ...
Testing the StackLazy algorithm ...
Testing the StackEager algorithm ...
Testing the StackProjective algorithm with pseudo-projective parsing (PP) ...
New best algorithm: stackproj + pp option
Incremental LAS improvement: + 0.010% (85.73%)
Testing the NivreStandard algorithm with pseudo-projective parsing (PP) ...
Best Non-Projective algorithm: StackProjective

-----------------------------------------------------------------------------
MaltOptimizer found that the best parsing algorithm is: stackproj+ pp option
Testing pseudo-projective (PP) options ...
Incremental improvement over the baseline at the end of Phase 2: +0.010% (85.73%) 
-----------------------------------------------------------------------------
MaltOptimizer has completed the parsing algorithm selection phase for your
training set and saved the results for future use in phase2_logFile.txt. 
Updated MaltParser options can be found in phase2_optFile.txt. If you want
to change any of these options, you should edit phase2_optFile.txt before.
you start the next optimization phase.

To proceed with Phase 3 (Feature Selection) run the following command:
java -jar MaltOptimizer.jar -p 3 -m <malt_path> -c <trainingCorpus>
-----------------------------------------------------------------------------
                   MaltOptimizer 1.0
-----------------------------------------------------------------------------
         Miguel Ballesteros* and Joakim Nivre**

          *Complutense University of Madrid (Spain)  
                **Uppsala University (Sweden)   
-----------------------------------------------------------------------------
PHASE 3: FEATURE SELECTION

MaltOptimizer is going to perform the following feature selection experiments:
1. Tune the window of POSTAG n-grams over the parser state.
2. Tune the window of FORM features over the parser state.
3. Tune DEPREL and POSTAG features over the partially built dependency tree.
4. Add POSTAG and FORM features over the input string.
5. Add CPOSTAG, FEATS, and LEMMA features if available.
6. Add conjunctions of POSTAG and FORM features.
-----------------------------------------------------------------------------
1. Tuning the window of POSTAG n-grams ... 

  rm InputColumn(POSTAG,Stack[2])
  add InputColumn(POSTAG, Stack[3])
New best feature model: forwardStackPostag1.xml
Incremental LAS improvement: + 0.079% (85.81%)
  add InputColumn(POSTAG, Stack[4])
New best feature model: forwardStackPostag2.xml
Incremental LAS improvement: + 0.140% (85.95%)
  add InputColumn(POSTAG, Stack[5])
  rm InputColumn(POSTAG,Lookahead[2])
  add InputColumn(POSTAG, Lookahead[3])

Best feature model: forwardStackPostag2.xml
-----------------------------------------------------------------------------
2. Tuning the window of FORM features ... 

  rm InputColumn(FORM,Stack[1])
  add InputColumn(FORM, Stack[2])
New best feature model: forwardStackForm1.xml
Incremental LAS improvement: + 0.079% (86.03%)
  add InputColumn(FORM, Stack[3])
New best feature model: forwardStackForm2.xml
Incremental LAS improvement: + 0.079% (86.11%)
  add InputColumn(FORM, Stack[4])
  rm InputColumn(FORM,Lookahead[0])
  add InputColumn(FORM, Lookahead[1])
  add InputColumn(FORM, head(Stack[0]))


Best feature model: forwardStackForm2.xml
-----------------------------------------------------------------------------
3. Tuning dependency tree features ... 

  rm Merge3(InputColumn(POSTAG, Stack[0]), OutputColumn(DEPREL, ldep(Stack[0])), OutputColumn(DEPREL, rdep(Stack[0])))
  rm Merge3(InputColumn(POSTAG, Stack[1]), OutputColumn(DEPREL, ldep(Stack[1])), OutputColumn(DEPREL, rdep(Stack[1])))
  add InputColumn(POSTAG, ldep(Stack[0]))
New best feature model: replicateDeprelPostagSubtractingFeature1.xml
Incremental LAS improvement: + 0.140% (86.25%)
  add InputColumn(POSTAG, rdep(Stack[0]))
  add InputColumn(POSTAG, ldep(Stack[1]))

Best feature model: replicateDeprelPostagSubtractingFeature1.xml
-----------------------------------------------------------------------------
4. Adding string features ... 

  add InputColumn(POSTAG,pred(Stack[0]))
  add InputColumn(POSTAG,succ(Stack[0]))
  add InputColumn(POSTAG,pred(Stack[1]))
  add InputColumn(POSTAG,succ(Stack[1]))

Best feature model: replicateDeprelPostagSubtractingFeature1.xml
-----------------------------------------------------------------------------
5. Adding CPOSTAG, FEATS, and LEMMA features ... 

Adding CPOSTAG features ...
  add InputColumn(CPOSTAG, Stack[0])
New best feature model: addLookaheadCPOSTAG0.xml
Incremental LAS improvement: + 0.319% (86.57%)
  add InputColumn(CPOSTAG, Lookahead[0])
New best feature model: addLookaheadCPOSTAG1.xml
Incremental LAS improvement: + 0.160% (86.73%)
  add InputColumn(CPOSTAG, Lookahead[1])
  add InputColumn(CPOSTAG, Stack[1])
New best feature model: addStackCPOSTAG0.xml
Incremental LAS improvement: + 0.329% (87.06%)
  add InputColumn(CPOSTAG, Stack[2])

Adding LEMMA features ...
  add InputColumn(LEMMA, Stack[0])
  add InputColumn(LEMMA, Stack[1])
New best feature model: addStackLEMMA0.xml
Incremental LAS improvement: + 0.129% (87.19%)
  add InputColumn(LEMMA, Stack[2])

Best feature model: addStackLEMMA0.xml
-----------------------------------------------------------------------------
6. Adding conjunctions of POSTAG and FORM features... 

  add Merge(InputColumn(POSTAG, Lookahead[0]), InputColumn(FORM, Lookahead[0]))
  add Merge(InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Lookahead[0])
  add Merge(InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[1])
  add Merge(InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[2])
  add Merge(InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[3])
Could not open system output file /nfs/guille/tadepalli/students/mannemp/EWS/MaltParser/MaltOptimizer/MaltOptimizer-1.0.3/outstackproj.conll
Feature not valid.
  add Merge(InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[4])
Could not open system output file /nfs/guille/tadepalli/students/mannemp/EWS/MaltParser/MaltOptimizer/MaltOptimizer-1.0.3/outstackproj.conll
Feature not valid.
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Lookahead[0]))
New best feature model: addMergPOSTAGI0FORMLookahead0.xml
Incremental LAS improvement: + 0.079% (87.27%)
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Stack[1]))
Could not open system output file /nfs/guille/tadepalli/students/mannemp/EWS/MaltParser/MaltOptimizer/MaltOptimizer-1.0.3/outstackproj.conll
Feature not valid.
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Stack[2]))
New best feature model: addMergPOSTAGI0FORMStack1.xml
Incremental LAS improvement: + 0.060% (87.33%)
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Stack[3]))
  add Merge(InputColumn(POSTAG, Stack[0]), InputColumn(FORM, Stack[4]))
  add Merge3(InputColumn(POSTAG, Stack[0]), InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Lookahead[0]))
  add Merge3(InputColumn(POSTAG, Stack[0]), InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[1]))
New best feature model: addMergPOSTAGS0I0FORMStack0.xml
Incremental LAS improvement: + 0.239% (87.57%)
  add Merge3(InputColumn(POSTAG, Stack[0]), InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[2]))
  add Merge3(InputColumn(POSTAG, Stack[0]), InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[3]))
  add Merge3(InputColumn(POSTAG, Stack[0]), InputColumn(POSTAG, Stack[1]), InputColumn(FORM, Stack[4]))

Best feature model: addMergPOSTAGS0I0FORMStack0.xml
-----------------------------------------------------------------------------
MaltOptimizer has concluded feature selection and is going to tune the SVM cost parameter.

Testing: C=0.01
85.86(Best:87.57)
Testing: C=0.2
86.96(Best:87.57)
Testing: C=0.4
86.21(Best:87.57)
Testing: C=0.6
85.74(Best:87.57)
Testing: C=0.8
85.39(Best:87.57)

Best C value: 0.1
Incremental improvement over the baseline at the end of Phase 3: + 1.849% (87.57)
-----------------------------------------------------------------------------
MaltOptimizer has completed the feature model testing phase using your training set,
it saved the results for future use in phase3_logFile.txt. Updated MaltParser 
options can be found in phase3_optFile.txt.

